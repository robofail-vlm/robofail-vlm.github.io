def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
    # Compute the distance between the TCP (tool center point) and the handle
    tcp_to_handle_dist = torch.linalg.norm(
        info["handle_link_pos"] - self.agent.tcp.pose.p, axis=1
    )
    reaching_reward = 1 - torch.tanh(5 * tcp_to_handle_dist)
    reward = reaching_reward

    # Compute the reward for grasping the handle
    is_grasped = self.agent.is_grasping(self.handle_link)
    reward += is_grasped


    # Compute the distance the drawer has been pulled out
    open_frac = (self.handle_link.joint.qpos - self.handle_link.joint.limits[..., 0]) / (
        self.handle_link.joint.limits[..., 1] - self.handle_link.joint.limits[..., 0]
    )
    open_reward = 1 - torch.tanh(5 * (self.min_open_frac - open_frac))
    reward += open_reward * is_grasped


    # Encourage a progressive motion of opening the drawer: add positive reward only if drawer's open_frac is increasing
    drawer_change = open_frac - getattr(self, '_prev_open_frac', open_frac)
    progressive_reward = drawer_change.where(drawer_change > 0, torch.tensor(0.0))
    reward += progressive_reward * 10  # Multiplier adjusted to emphasize its importance
    self._prev_open_frac = open_frac


    # Compute the static reward to encourage the agent to keep the drawer open
    link_is_static = (
        torch.linalg.norm(self.handle_link.angular_velocity, axis=1) <= 1
    ) & (torch.linalg.norm(self.handle_link.linear_velocity, axis=1) <= 0.1)
    static_reward = 1 - torch.tanh(5 * (1 - link_is_static.float()))
    reward += static_reward * info["open_enough"]


    # Penalty for y-direction offset movement after grasping
    y_offset_penalty = torch.abs(self.agent.tcp.pose.p[:, 1] - info["handle_link_pos"][:, 1])
    reward -= y_offset_penalty * is_grasped


    # Add a large reward if the task is successfully completed
    reward[info["success"]] = 5.0
    return reward
