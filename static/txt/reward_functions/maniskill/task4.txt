def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
    # Distance from the TCP (Tool Center Point) to cubeA
    tcp_to_cubeA_dist = torch.linalg.norm(
    self.cubeA.pose.p - self.agent.tcp.pose.p, axis=1
    )
    reaching_reward = 1 - torch.tanh(5 * tcp_to_cubeA_dist)
    reward = reaching_reward

    # Reward for grasping cubeA
    is_cubeA_grasped = info["is_cubeA_grasped"]
    reward += is_cubeA_grasped

    # Distance from cubeA to cubeB along the x and y directions (ignore z)
    cubeA_to_cubeB_dist_xy = torch.linalg.norm(
    self.cubeB.pose.p[..., :2] - self.cubeA.pose.p[..., :2], axis=1
    )
    stacking_reward_xy = 1 - torch.tanh(5 * cubeA_to_cubeB_dist_xy)
    reward += stacking_reward_xy * is_cubeA_grasped

    # Penalty for misalignment in the z direction
    z_offset_penalty = torch.abs(self.cubeB.pose.p[..., 2] - self.cubeA.pose.p[..., 2])
    reward -= z_offset_penalty * is_cubeA_grasped

    # Reward for keeping cubeA static
    is_cubeA_static = info["is_cubeA_static"]
    static_reward = 1 - torch.tanh(5 * torch.linalg.norm(self.agent.robot.get_qvel()[..., :-2], axis=1))
    reward += static_reward * info["is_cubeA_on_cubeB"]

    # Final success reward
    reward[info["success"]] = 8
    return reward